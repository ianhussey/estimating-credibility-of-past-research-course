---
title: 'Quantifying Support for the Null Hypothesis in Psychology: An Empirical Investigation'
author: Aczel, B., Palfi, B., Szollosi, A., Kovacs, M., Szaszi, B., Szecsi, P., Zrubka,
  M., Gronau, Q. F., van den Bergh, D., & Wagenmakers, E.-J.
date: "2017 November 20"
output:
  html_document: default
  pdf_document: default
---

# Loading libraries
```{r, warning=FALSE, message=FALSE}
library(psych)
library(ggplot2)
library(reshape2)
library(BayesFactor)
library(gmodels)
library(dplyr)
library(stringr)
library(stats)
library(Hmisc)
library(hypergeo)
library(DescTools)
```

# Additional functions
```{r}
## Bayes factor
### Source: https://gist.github.com/richarddmorey/3dbd911466389f7f263e
### for paired samples
p.y.alt_p = function(t.stat, N, log.prior.dens,lo=-Inf,up=Inf,...){
  normalize = integrate(function(delta,...){
    exp(log.prior.dens(delta, ...))
  }, lower = lo, upper = up, ...)[[1]]
  py = integrate(function(delta,t.stat,N,...){
    exp(
      dt(t, N - 1, ncp = delta*sqrt(N), log = TRUE) +
        log.prior.dens(delta, ...)  
    )
  },lower = lo, upper = up, t.stat = t.stat, N = N,stop.on.error = FALSE, ...)[[1]]
  py/normalize
}

### for independent samples / ess = effective smaple size
p.y.alt_i = function(t.stat, n1, n2, log.prior.dens,lo=-Inf,up=Inf,...){
  normalize = integrate(function(delta,...){
    exp(log.prior.dens(delta, ...))
  }, lower = lo, upper = up, ...)[[1]]
  py = integrate(function(delta,t.stat,n1,n2,...){
    ess = n1*n2 / (n1+n2)
    exp(
      dt(t, n1+n2 - 2, ncp = delta*sqrt(ess), log = TRUE) +
        log.prior.dens(delta, ...)  
    )
  },lower = lo, upper = up, t.stat = t.stat, n1=n1, n2=n2,stop.on.error = FALSE, ...)[[1]]
  py/normalize
}

### t.test
B01_paired = function(t.stat, N, log.prior.dens, lo = -Inf, up = Inf, ...){
  dt(t.stat, N - 1)/p.y.alt_p(t,N,log.prior.dens,lo,up,...)
}

B01_independent = function(t.stat, n1,n2, log.prior.dens, lo = -Inf, up = Inf, ...){
  dt(t.stat, n1+n2 - 2)/p.y.alt_i(t,n1,n2,log.prior.dens,lo,up,...)
}

## Prior of the Bayes factor
### Normal and Cauchy prior functions 
normal.prior = function(delta, mu=0, sd=1){
  dnorm(delta, mu, sd, log = TRUE)
}

cauchy.prior = function(delta, rscale){
  dcauchy(delta, scale = rscale, log = TRUE)
}

### Informed prior
source("informedTtest_functions.R")

## Calculating Kendalls tau
### Source: https://osf.io/b9qhj/
source("R_Code_KendallBayesFactor.R")
```
 
# Data
```{r}
## Our data
sample_nonsig <- read.csv2("data.csv")

## Hartgerink`s data: https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:65072
statcheck_nonsig <- read.csv("Statcheck_nonsig_p.csv")
```

# Descriptives
```{r}
## Change variable types
sample_nonsig <- 
  sample_nonsig %>% mutate(
    Reference=as.character(Reference),
    Link=as.character(Link),
    t=as.numeric(as.character(t)),
    Associated.statistics=as.character(Associated.statistics),
    p=as.numeric(as.character(p)),
    Negative.statement=as.character(Negative.statement))

## Abstracts with negative statements
### Number of all abstracts with negative statement
sample_nonsig %>% select(Article.ID) %>% 
  summary()

### Number of abstracts per journal
sample_nonsig %>%
  group_by(Journal.ID) %>% 
  distinct(Article.ID) %>% 
  summarise(length(Journal.ID))

### Number of all negative statements
sample_nonsig %>% 
  distinct(Negative.statement) %>% 
  summarise(n = n())

### Number of negative statements per journal
sample_nonsig %>%
  group_by(Journal.ID) %>%
  distinct(Negative.statement) %>%
  summarise(n = n())

## Number of nonsignificant p-values
sample_nonsig %>% 
  filter(is.na(p) == FALSE) %>% 
  summarise(n = n())

## Number of nonsignificant p-values per journal
sample_nonsig %>% 
  group_by(Journal.ID) %>%
  filter(is.na(p) == FALSE) %>% 
  summarise(length(p))
```


# Analysis of the whole Nonsignificant sample
```{r}
## Categories
levels(sample_nonsig$Category)

## Frequency of statements by categories
sample_nonsig %>% 
  group_by(Category) %>% 
  distinct(Negative.statement) %>% 
  summarise(n = n())

## Counting the number of categories within each journal
sample_nonsig %>%
  group_by(Journal.ID, Category) %>% 
  distinct(Negative.statement) %>%
  summarise(n = n())
```

# Bayes Analyses
## Data management
```{r}
### Number of related t-test per journal
levels(sample_nonsig$Type.of.statistics)

ttests <- c("One Sample T-Test", "Paired Samples T-Test", "Independent Samples T-Test")

sample_nonsig %>% 
  filter(Type.of.statistics %in% ttests) %>% 
  group_by(Type.of.statistics) %>%
  summarise(length(Type.of.statistics))

### Subsetting the data
nonsigB<- subset(sample_nonsig, is.na(sample_nonsig$t)!=TRUE&is.na(sample_nonsig$N1)!=TRUE) 
#### Clearing target variables
nonsigB$BF01default <- NULL
nonsigB$BF01informed <- NULL
nonsigB$BF01normal <- NULL

### Transforming NAs to 0
nonsigB$N2 <- ifelse(is.na(nonsigB$N2)==TRUE,0,nonsigB$N2)
#### Sample size
nonsigB$N <- nonsigB$N1 + nonsigB$N2
#### Design: 0 = paired, 1 = independent
nonsigB$Design <- ifelse(nonsigB$N2==0, 0,1)

### Testing whether the original statistics was one-sided or two-sided
nonsigB$p.calculated <- 2*pt(abs(nonsigB$t), ifelse(nonsigB$Design==0, nonsigB$N1-1, (nonsigB$N1+nonsigB$N2 -2)), lower = FALSE)
nonsigB$OneTailed <- ifelse(0.01+nonsigB$p.calculated/2<nonsigB$p, 0,1)

sum(nonsigB$OneTailed) # 4 cases where p value is One-tailed in the paper
```

## Bayes factor calculation
```{r, results='hide', warning=FALSE, message=FALSE}
### Calculating Bayes factor
for (i in 1:nrow(nonsigB)){
  nonsigB[i,"BF01default"] <-1/exp(ttest.tstat(t = nonsigB[i, "t"],
                                      n1 = nonsigB[i, "N1"],
                                      n2 = nonsigB[i, "N2"],
                                      nullInterval = NULL,
                                      rscale = "medium",
                                      complement = FALSE,
                                      simple = FALSE)[['bf']])
    
### Bayes factor with normal prior
t = nonsigB[i, "t"]
N1 = nonsigB[i, "N1"]
N2 = nonsigB[i, "N2"]
N = nonsigB[i, "N1"]
  
ifelse(test = nonsigB[i, "Design"]==0,
        nonsigB[i, "BF01normal"] <- B01_paired(t, N, normal.prior,-Inf,Inf,mu=0,sd=0.5),
        nonsigB[i, "BF01normal"] <- B01_independent(t, N1,N2, normal.prior,-Inf,Inf,mu=0,sd=0.5))
  }

### Bayes factor with informed prior
#### Informed prior
prior.location1 <- - 0.34999
prior.location2 <- 0.34999
prior.scale <- 0.1021
prior.df <- 3

for (i in seq_len(nrow(nonsigB))) {
  
  print(i)
  
  if (!is.na(nonsigB$t[i]) && !is.na(nonsigB$N1[i]) &&
      nonsigB$Type.of.statistics[i] %in% c("One Sample T-Test", "Paired Samples T-Test")) {
    
    BF10_1 <- bf10_t(nonsigB$t[i], ny = nonsigB$N1[i],
                     prior.location = prior.location1, prior.scale = prior.scale,
                     prior.df = prior.df)$BF10
    BF10_2 <- bf10_t(nonsigB$t[i], ny = nonsigB$N1[i],
                     prior.location = prior.location2, prior.scale = prior.scale,
                     prior.df = prior.df)$BF10
    nonsigB$BF01informed[i] <- 1/mean(c(BF10_1, BF10_2))
    
  } else if (!is.na(nonsigB$t[i]) && !is.na(nonsigB$N1[i]) && !is.na(nonsigB$N2[i]) &&
             nonsigB$Type.of.statistics[i] == "Independent Samples T-Test") {
    
    BF10_1 <- bf10_t(nonsigB$t[i], ny = nonsigB$N1[i], nx = nonsigB$N2[i], independentSamples = TRUE,
                     prior.location = prior.location1, prior.scale = prior.scale,
                     prior.df = prior.df)$BF10
    BF10_2 <- bf10_t(nonsigB$t[i], ny = nonsigB$N1[i], nx = nonsigB$N2[i], independentSamples = TRUE,
                     prior.location = prior.location2, prior.scale = prior.scale,
                     prior.df = prior.df)$BF10
    nonsigB$BF01informed[i] <- 1/mean(c(BF10_1, BF10_2))
    
  }}
```

## Data management: logBF
```{r}
### Producing log Bayes factors with reciprocal values
#### ln Bayes factors
nonsigB$logBF01default <- log(nonsigB$BF01default)
nonsigB$logBF01normal <- log(nonsigB$BF01normal)
nonsigB$logBF01informed <- log(nonsigB$BF01informed)

#### ln sample size
nonsigB$logN <- log(nonsigB$N)
```

## Proportions of default Bayes factors in the evidence categories
```{r}
### Proportion of evidence categories
nonsigB$Evidence_def <- ifelse(nonsigB$BF01default<3, "Anecdotal", ifelse(nonsigB$BF01default<10, "Moderate", "Strong"))
nonsigB$Evidence_inf <- ifelse(nonsigB$BF01informed<3, "Anecdotal", ifelse(nonsigB$BF01informed<10, "Moderate", "Strong"))
nonsigB$Evidence_nrm <- ifelse(nonsigB$BF01normal<3, "Anecdotal", ifelse(nonsigB$BF01normal<10, "Moderate", "Strong"))

CrossTable(nonsigB$Evidence_def)
```

## Results of robustness test
```{r}
# Change in the proportion of evidence category between default and informed
nonsigB %>% 
  filter(Evidence_def != Evidence_inf) %>% 
  summarise(n=n())

# Change in the proportion of evidence category between default and normal
nonsigB %>% 
  filter(Evidence_def != Evidence_nrm) %>% 
  summarise(n=n())
```

## Plot of robustness test
```{r, fig.keep='high'}
pdf(file ="D://Dropbox//Nonsignificant//Publishing//Publishing//Figures//robustness.pdf", width = 8, height = 6)


###Organizing data
dat = nonsigB
#head(dat)

x = seq_len(nrow(dat))
xl = dat$Article.ID

y = as.matrix(dat[, c("logBF01normal", "logBF01default", "logBF01informed")])
colnames(y) <- c("logBF01normal", "logBF01default", "logBF01informed") #change medium to default
#sort articles from lowest to highest BF based on BF01default
#highest = which(y == max(y), arr.ind = TRUE)[1, 2]
ord = order(y[, 2])
y = y[ord, ]
xl = x[ord]

###Grayscale
cols = rep("black", 3)
colsBG = rep("gray80", 3)

###Adjust opacity
colsA = scales::alpha(cols, .6)
colsBGA = scales::alpha(colsBG, .6)

###Labels
xlab = "Study"
ylab = "Bayes Factors in Favor of the Null"
cex.lab = 1 # scaling on axis labels
pch = c(21, 22, 24) # circle, square, triangle
lwd = 1     # other lwd
lwdP = 1    # thickness of ring around points
lwdArr = 1 # arrows
pt.cex = 1 # size of points

###Plot with percentages
dataLim = range(c(y))

###Indices we want to keep
yLBreaks = c(log(c(100, 30, 10, 3, 1)), -rev(log(c(100, 30, 10, 3))))

idx = findInterval(yLBreaks, dataLim) == 1
yLBreaks = yLBreaks

yRBreaks = c(yLBreaks[yLBreaks >= 0] + .602, yLBreaks[yLBreaks <= 0] - .602)
yRlabels = c("Extreme H0","Very strong H0", "Strong H0", "Moderate H0", "Anecdotal H0", "Anecdotal H1", "Moderate H1", "Strong H1", "Very strongH1", "Extreme H1")
#yRlabels = c(yRlabels, rev(yRlabels))#[c(FALSE, idx, rep(FALSE, 5))]
yLlabels = rev(c("1/100", "1/30", "1/10", "1/3", "1", "3", "10", "30", "100"))

ylim = range(yLBreaks[idx]) + c(-1, 1) # so extreme doesn't fall of the plot area

###Par from R compendium
par(cex.main = 1, mar = c(2.5, 2.5, 1.5, 6.5) + 0.1, mgp = c(1.5, 0.5, 0), cex.lab = 1,
	font.lab = 1, cex.axis = 1, las = 1)

###Empty plot
plot(1, 1, cex.lab = 1, font.lab =1, type = "n", xlim = c(0, 68), ylim = ylim,
	 ylab = ylab, xlab = xlab, axes = FALSE)

###Add horizontal gridlines
abline(h = yLBreaks, lty = 2, col = "gray", lwd = 2)

###Add points
matpoints(x, y, type = 'b', col = colsA, bg = colsBGA,
		  pch = pch, lty = 3, lwd = lwd, cex = pt.cex)

###Add axes
axis(1, at = c(1, seq(10, max(x), 10), max(x)))#, cex.axis = tickCex)
axis(2, yLBreaks, las = 1, labels = yLlabels)#, cex.axis = tickCex)
axis(side = 4, at = yLBreaks, tick = TRUE, labels = FALSE)
axis(side = 4, at = yRBreaks, tick = FALSE, labels = yRlabels)
#grid::grid.text("Evidence", 0.98, 0.5, rot = 270, gp = grid::gpar(cex = cex.lab))

###Add percentages
percN = table(cut(y[, 1], breaks = yLBreaks)) / nrow(y)
percM = table(cut(y[, 2], breaks = yLBreaks)) / nrow(y)
percU = table(cut(y[, 3], breaks = yLBreaks)) / nrow(y)

perc = formatC(paste0(round(100*c(percN, percM, percU), 1), "%"))
#perc = paste(formatC(100*c(percN, percM, percU), digits = 3), "%")
perc = paste0(ifelse(nchar(gsub(" ", "", perc)) == 2, " ", ""), perc) # add some spaces so numbers are right aligned

yy = rowMeans(cbind(yLBreaks[-length(yLBreaks)], yLBreaks[-1]))
text(x = rep(65, 9), y = rep(rev(yy), 3) + rep(c(-.35, .35, 0), each = length(yy)), cex = 0.8,
	 labels = perc, col = rep(cols, each = length(yy)))

###Add legend
ord = c(2, 3, 1) # legend order
nms = unlist(strsplit(colnames(y), "logBF01"))
nms = paste(nms[nchar(nms) > 0], "prior")
substr(nms, 1, 1) = toupper(substr(nms, 1, 1))

legend("topleft", legend = nms[ord], lty = 1,
	   col = cols[ord], pch = pch[ord], pt.bg = colsBG[ord],
	   bty = "n", cex = 1, lwd = lwd, pt.cex = pt.cex,
	   inset = c(0, 1.75e-2*diff(yLBreaks)[1]), y.intersp = 1)

###Add three single points so percentages make sense in grayscales
points(rep(69.5, 3), rep(yy[1], 3) + c(.35, 0, -.35),
	   col = cols[ord], pch = pch[ord], bg = colsBG[ord], lwd = lwd, cex = pt.cex)

dev.off()
```


# Non-registered analyses
## B against p plot
```{r, fig.keep='high'}
### The code for Figure 2 was adapted from: http://shinyapps.org/apps/RGraphCompendium/index.php#evidential-flow
pdf(file ="D://Dropbox//Nonsignificant//Publishing//Publishing//Figures//BF_p.pdf", width = 8, height = 6)

par(cex.main = 1.3, mar = c(4.5, 6, 4, 7) + 0.1, mgp = c(3, 1, 0), cex.lab = 1.3, 
    font.lab = 2, cex.axis = 1.3, las = 1)

plot(x= nonsigB[,'p'], y=nonsigB[,'logBF01default'], xlim = c(0, 1), ylim = c(-1 * log(4), log(30)), 
     xlab = "", ylab = "", cex.lab = 1.3, cex.axis = 1.3, las = 1, yaxt = "n",xaxt="n", cex = 1, 
     bty = "n", type = "p", pch = 21, bg = "grey")

labelsUpper = log(c(30, 10, 3, 1))
lablesUppercorx = log(c(10, 3, 1))
labelsLower = -1 * log(c(3, 1))
criticalP = c(labelsLower, 0, labelsUpper)
criticalPcorx = c(lablesUppercorx,0,labelsLower)
abline(h = 0)
axis(side = 4, at = labelsUpper + 0.602, tick = FALSE, cex.axis = 1, labels = c ( "", "Strong H0", "Moderate H0", "Anecdotal H0"))
axis(side = 4, at = labelsLower - 0.602, tick = FALSE, cex.axis = 1, labels = c( "", "Anecdotal H1"))
axis(side = 2, at = c(criticalP), tick = TRUE, las = 2, cex.axis = 1, labels = c( "1/3", "1", "",  "30", "10", "3", ""))
axis(side=1, at= c(0,0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.60,0.65,0.70,0.75,0.80,0.85,0.90,0.95,1), cex.axis= 0.7)
mtext("Bayes Factors in Favor of the Null", side = 2, line = 2.5, las = 0, cex = 1)
mtext("P-values", side = 1, line = 2.5, las = 1, cex = 1)
grid::grid.text("", 0.97, 0.5, rot = 270, gp = grid::gpar(cex = 1.3))
for (idx in 1:length(criticalPcorx)) {
  abline(h = criticalPcorx[idx], col = "darkgrey", lwd = 1, lty = 2)
}

dev.off()
```

## Correlation for B against P
```{r}
Bs <- nonsigB$BF01default
p_values <- nonsigB$p
yourN <- length(p_values)

# yourKendallTauValue <- cor(Bs, p_values, method = "kendall")
yourKendallTauValue <- KendallTauB(Bs, p_values, conf.level = 0.95)[1]

bfCorrieKernelKendallTau(tau = yourKendallTauValue, n = yourN)

credibleIntervalKendallTau(kentau = yourKendallTauValue, n = yourN)

plot(density(sampleTausA(myTau = yourKendallTauValue, myN = yourN),from = -1, to = 1), las = 1, bty = "n", lwd=3, 
     main = "Posterior Distribution for Kendall's tau", xlab = expression(tau))
```

## B against N
```{r, fig.keep='high'}
pdf(file ="D://Dropbox//Nonsignificant//Publishing//Publishing//Figures//BF_N.pdf",width = 8, height = 6)

# The code for Fig. S2 was adapted from: http://shinyapps.org/apps/RGraphCompendium/index.php#evidential-flow
par(cex.main = 1, mar = c(4.5, 6, 4, 7) + 0.1, mgp = c(3, 1, 0), cex.lab = 1, 
    font.lab = 2, cex.axis = 1, las = 1)
plot(x= nonsigB[,'logN'], y=nonsigB[,'logBF01default'], xlim = c(log(10), log(1000)), ylim = c(-1 * log(4), log(30)), 
     xlab = "", ylab = "", cex.lab = 1, cex.axis = 1, las = 1, yaxt = "n",xaxt="n", cex = 1, 
     bty = "n", type = "p", pch = 21, bg = "grey")

labelsUpper = log(c(30, 10, 3, 1))
lablesUppercorx = log(c(10, 3, 1))
labelsLower = -1 * log(c(3, 1))
criticalP = c(labelsLower, 0, labelsUpper)
criticalPcorx = c(lablesUppercorx,0,labelsLower)
axis(side = 4, at = labelsUpper + 0.602, tick = FALSE, cex.axis = 1, labels = c ( "", "Strong H0", "Moderate H0", "Anecdotal H0"))
axis(side = 4, at = labelsLower - 0.602, tick = FALSE, cex.axis = 1, labels = c( "", "Anecdotal H1"))
axis(side = 2, at = c(criticalP), tick = TRUE, las = 2, cex.axis = 1, labels = c( "1/3", "1", "",  "30", "10", "3", ""))
axis(side=1, at= c(log(10),log(30),log(100),log(300),log(1000)), cex.axis= 0.7, labels = c("10","30","100","300","1000"))
mtext("Bayes Factors in Favor of the Null", side = 2, line = 2.5, las = 0, cex = 1)
mtext("Sample Size", side = 1, line = 2.5, las = 1, cex = 1)
grid::grid.text("", 0.97, 0.5, rot = 270, gp = grid::gpar(cex = 1.3))
for (idx in 1:length(criticalPcorx)) {
  abline(h = criticalPcorx[idx], col = "darkgrey", lwd = 1, lty = 2)
}
abline(h = 0)

dev.off()
```

## Correlation for B against N
```{r}
Bs <- nonsigB$BF01default
N <- nonsigB$N
yourN <- length(N)

# yourKendallTauValue <- cor(Bs, N, method = "kendall")
yourKendallTauValue <- KendallTauB(Bs, N, conf.level = 0.95)[1]

bfCorrieKernelKendallTau(tau = yourKendallTauValue, n = yourN)

credibleIntervalKendallTau(kentau = yourKendallTauValue, n = yourN)

plot(density(sampleTausA(myTau = yourKendallTauValue, myN = yourN),from = -1, to = 1), las = 1, bty = "n", lwd=3, 
     main = "Posterior Distribution for Kendall's tau", xlab = expression(tau))

nonsigB_smallN <- subset(nonsigB, nonsigB$N < 35)

# Relationship between small sample size and evidence catagories
nonsigB %>% 
  group_by(Evidence_def) %>% 
  select(N) %>% 
  filter(N < 35) %>% 
  summarise(n=n())

# Relationship between strong evidence and N
nonsigB %>%
  group_by(Evidence_def) %>% 
  filter(Evidence_def == "Strong") %>% 
  select(N, Article.ID, Statement.ID, Stat.ID)
```

# Supplementary Materials

## P distributions
```{r}
## Hartgerink`s dataset
statcheck_nonsig <- statcheck_nonsig %>% 
  select(p = Computed) %>% 
  mutate(Sample = "All Nonsignificant P-values")

## Our sample
our_nonsig <- sample_nonsig %>%
  select(p) %>% 
  mutate(Sample = paste0("Nonsignificant P-values", "\n", "for Negative Claim in the Abstract")) %>% 
  filter(is.na(p)!=TRUE) #122 p-values

## Merge
nonsig <- rbind(statcheck_nonsig, our_nonsig)
nonsig$p <- as.numeric(nonsig$p)

## Descriptives for Figure 1. note
nonsig %>% 
  group_by(Sample) %>% 
  summarise(n=n())
```

## Plotting the histogram of p-values
```{r, fig.keep='high'}
# Merged Figure
pdf(file ="D://Dropbox//Nonsignificant//Publishing//Publishing//Figures//P value distribution.pdf", width = 8, height = 6) #use this line to save the plot

# attempt recreate the ggplot2 transparancy.
cols <- ggplot2::alpha(c("#A0A0A0","#000000"), .6) 

# x-axis tick labels
xBreaks <- c(0, 0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)

# bins identical to ggplot version but substracted 1e-6 to avoid that 0.05 falls into the bin 0.03-0.05.
bins <- c(-0.01, seq(.01, .99, .02), 1.01) - 1e-6 # using 0 instead of -0.01 gives a density plot

# rotate y-axis tick-labels, thickness of axes and labels.
par(las = 1, cex.axis = 1.2, cex.lab = 1.5)

# draw first histogram
h1 <- hist(nonsig$p, col = cols[1], breaks = bins, ylab = "Count", axes = FALSE, xlab = "P-value", main = "",
		   ylim = c(0, 70))

# add axes
axis(side = 1, at = xBreaks)
axis(side = 2, at = pretty(c(0, 70), n = 5))

# add x-axis 0.05 labels
oldMpg <- par("mgp")
par(mgp = c(3, 2.5, 0))
axis(side = 1, at = .05, labels = "0.05")
par(mgp = oldMpg) 

# add second histogram
hist(subset(nonsig$p, nonsig$Sample == paste0("Nonsignificant P-values", "\n", "for Negative Claim in the Abstract")), col = cols[2], breaks = bins, add = TRUE)

# add legend
legend("topright", legend = unique(nonsig$Sample), col = cols, bty = "n", pch = 15, cex = 1, y.intersp = .65, adj = c(0,.75))

dev.off()
```

